import sys
import time
import math
import json
import torch
import pynvml
import bitsandbytes as bnb

from pathlib import Path
from datetime import datetime
from transformers import GPTNeoForCausalLM, GPT2Tokenizer, BitsAndBytesConfig

from model_utils import TrainingMode, ValidationMode, GenerationMode
from monitor_utils import AdvancedTrainingMonitor

MAX_STEP_TIME = 30  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —à–∞–≥–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("‚ö†Ô∏è  tqdm –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install tqdm")

HAS_PYNVML = False
try:
	import pynvml
	pynvml.nvmlInit()
	HAS_PYNVML = True
	pynvml.nvmlShutdown()
except:
	print("‚ö†Ô∏è  pynvml –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –º–æ—â–Ω–æ—Å—Ç—å GPU –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è")

def get_gpu_power():
	if not HAS_PYNVML:
		return "N/A"
	try:
		pynvml.nvmlInit()
		handle = pynvml.nvmlDeviceGetHandleByIndex(0)
		power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0
		pynvml.nvmlShutdown()
		return f"{power:.0f}"
	except:
		return "N/A"

print("=" * 80)
print("üß† –û–ë–£–ß–ï–ù–ò–ï –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ë–û–¢–ê - –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –í–ï–†–°–ò–Ø")
print("=" * 80)

# –î–û–ë–ê–í–¨–¢–ï –í–´–ë–û–† –†–ï–ñ–ò–ú–ê
print("\nüîß –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –≤—ã–≤–æ–¥–∞:")
print("1. üöÄ –ë—ã—Å—Ç—Ä—ã–π (—Ç–æ–ª—å–∫–æ LR, Loss, –ø—Ä–æ–≥—Ä–µ—Å—Å)")
print("2. üêõ –û—Ç–ª–∞–¥–æ—á–Ω—ã–π (–≤—Å–µ –¥–µ—Ç–∞–ª–∏)")
print("3. üìä –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π (–º–µ—Ç—Ä–∏–∫–∏ + –≥—Ä–∞—Ñ–∏–∫–∏)")
debug_mode = input("–í—ã–±–µ—Ä–∏—Ç–µ (1-3, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1): ").strip() or "1"
DEBUG_MODE = int(debug_mode)

print("=" * 80)
print("üß† –û–ë–£–ß–ï–ù–ò–ï –ü–°–ò–•–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ë–û–¢–ê - –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –í–ï–†–°–ò–Ø")
print("   –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò –ò –ú–ï–¢–†–ò–ö–ê–ú–ò")
print("=" * 80)

print("\nüîß –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
print("1. üöÄ 16-–±–∏—Ç–Ω—ã–π (FP16, —Å–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å) - –±—ã—Å—Ç—Ä–µ–µ, —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å")
print("2. üêò 32-–±–∏—Ç–Ω—ã–π (FP32, –ø–æ–ª–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å")
precision_choice = input("–í—ã–±–µ—Ä–∏—Ç–µ (1 –∏–ª–∏ 2, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1): ").strip() or "1"
PRECISION_MODE = int(precision_choice)  # 1 –¥–ª—è 16-–±–∏—Ç, 2 –¥–ª—è 32-–±–∏—Ç

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (AMP)
USE_AMP = PRECISION_MODE == 1
GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=USE_AMP)

print(f"\n   ‚úÖ –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º: {'16-–±–∏—Ç–Ω—ã–π (AMP)'if USE_AMP else '32-–±–∏—Ç–Ω—ã–π'} –æ–±—É—á–µ–Ω–∏—è")
print(f"   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 32-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (FP32)")

# ================= –ü–†–ê–í–ò–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ =================
BATCH_SIZE = 3 
MAX_LENGTH = 729
GRADIENT_ACCUMULATION = 6
LEARNING_RATE = 2e-4
EPOCHS = int(input("–í–≤–µ–¥–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö..."))
WARMUP_RATIO = 0.9

print("\nüéØ –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:")
print(f"   ‚Ä¢ Batch size: {BATCH_SIZE}")
print(f"   ‚Ä¢ Max length: {MAX_LENGTH}")
print(f"   ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION}")
print(f"   ‚Ä¢ Learning rate: {LEARNING_RATE:.1e}")
print(f"   ‚Ä¢ Epochs: {EPOCHS}")
print(f"   ‚Ä¢ Warmup: {WARMUP_RATIO*100}%")

# ================= –ö–û–ù–¢–ï–ö–°–¢–ù–´–ï –ú–ï–ù–ï–î–ñ–ï–†–´ –î–õ–Ø –†–ï–ñ–ò–ú–û–í =================

# ================= –ö–û–ù–°–¢–ê–ù–¢–´ =================
class TrainingConfig:
	"""–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã"""
	
	# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —à–µ–¥—É–ª–µ—Ä
	COSINE_DECAY_RATIO = 0.6      # 60% —à–∞–≥–æ–≤ –Ω–∞ cosine decay
	FINAL_LINEAR_START = 0.1      # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ LR –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ñ–∞–∑–µ
	FINAL_LINEAR_DECAY = 0.5      # –°–∫–æ—Ä–æ—Å—Ç—å –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ñ–∞–∑–µ
	
	# –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤
	MIN_WORDS = 5                 # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
	MAX_WORDS = 80                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
	UNIQUE_WORDS_RATIO = 0.6      # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
	MAX_EMPATHY_WORDS = 5         # –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ü–µ–Ω–∫–∏ —ç–º–ø–∞—Ç–∏–∏
	
	# –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
	MIN_DELTA = 0.001             # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–∏–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
	MAX_PATIENCE = 3              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π
	MAX_NAN_TOLERANCE = 3         # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ NaN –¥–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏
	
	# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
	BASE_TEMPERATURE = 0.729      # –ë–∞–∑–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
	MIN_TEMPERATURE = 0.6         # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
	MAX_TEMPERATURE = 0.9         # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
	TOP_P_HIGH = 0.95             # top_p –¥–ª—è –≤—ã—Å–æ–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
	TOP_P_LOW = 0.9               # top_p –¥–ª—è –Ω–∏–∑–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
	
	# –°—Ç—Ä—É–∫—Ç—É—Ä–∞
	MIN_SENTENCES = 2             # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
	MAX_SENTENCES = 5             # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
	MIN_WORDS_PER_SENTENCE = 5    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
	MAX_WORDS_PER_SENTENCE = 20   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏

# ================= –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –®–ï–î–£–õ–ï–† =================

class OptimalScheduler:
	"""
	–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —à–µ–¥—É–ª–µ—Ä –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
	Warmup ‚Üí Cosine Decay ‚Üí Linear Final
	"""
	
	def __init__(self, optimizer, total_steps, initial_lr, warmup_ratio):
		self.optimizer = optimizer
		self.total_steps = total_steps
		self.initial_lr = initial_lr
		self.warmup_steps = int(total_steps * warmup_ratio)
		self.cosine_steps = int(total_steps * 0.6)
		self.linear_steps = total_steps - self.warmup_steps - self.cosine_steps
		self.current_step = 0
		self.cosine_steps = int(total_steps * TrainingConfig.COSINE_DECAY_RATIO)
		
		print("\n üéØ –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –®–ï–î–£–õ–ï–† (3 —Ñ–∞–∑—ã):")
		print(f"   ‚Ä¢ –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")
		print(f"   ‚Ä¢ Warmup: {self.warmup_steps} —à–∞–≥–æ–≤ ({warmup_ratio*100}%)")
		print(f"   ‚Ä¢ Cosine decay: {self.cosine_steps} —à–∞–≥–æ–≤ ({TrainingConfig.COSINE_DECAY_RATIO*100}%)")
		print(f"   ‚Ä¢ Linear final: {self.linear_steps} —à–∞–≥–æ–≤ (–æ—Å—Ç–∞–ª—å–Ω–æ–µ)")
	
	def step(self):
		"""–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ —à–µ–¥—É–ª–µ—Ä–∞"""
		self.current_step += 1
		
		if self.current_step <= self.warmup_steps:
			# 1. Warmup: –ª–∏–Ω–µ–π–Ω—ã–π —Ä–æ—Å—Ç
			lr = self.initial_lr * (self.current_step / self.warmup_steps)
			phase = "WARMUP"
			
		elif self.current_step <= self.warmup_steps + self.cosine_steps:
			# 2. Cosine decay
			progress = (self.current_step - self.warmup_steps) / self.cosine_steps
			lr = self.initial_lr * 0.5 * (1 + math.cos(math.pi * progress))
			phase = "COSINE"
			
		else:
			# 3. –õ–∏–Ω–µ–π–Ω–æ–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ
			progress = (self.current_step - self.warmup_steps - self.cosine_steps) / self.linear_steps
			lr = self.initial_lr * TrainingConfig.FINAL_LINEAR_START * (1 - progress * TrainingConfig.FINAL_LINEAR_DECAY)
			phase = "FINAL"
		
		# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º LR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
		for param_group in self.optimizer.param_groups:
			param_group['lr'] = lr
		
		return lr, phase
# ================= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò =================

def check_scaler_health(scaler, context="—à–∞–≥"):
	"""–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è scaler –¥–ª—è AMP"""
	if not scaler._enabled:
		return True
	
	try:
		scale = scaler.get_scale()
		
		# –ö—Ä–∏—Ç–µ—Ä–∏–∏ –Ω–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
		if scale <= 0:
			print(f"   ‚ùå {context}: scale <= 0 ({scale:.2e})")
			return False
		if math.isnan(scale):
			print(f"   ‚ùå {context}: scale = NaN")
			return False
		if math.isinf(scale):
			print(f"   ‚ùå {context}: scale = –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—å")
			return False
		if scale > 1e6:  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
			print(f"   ‚ö†Ô∏è  {context}: scale —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({scale:.2e})")
			return False
		if scale < 1e-6:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π
			print(f"   ‚ö†Ô∏è  {context}: scale —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ({scale:.2e})")
			return False
			
		return True
		
	except Exception as e:
		print(f"   ‚ùå {context}: –æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ scaler: {e}")
		return False

def monitor_scaler_state(step, scaler, prefix=""):
	"""–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è scaler –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
	if not scaler._enabled:
		return
	
	scale = scaler.get_scale()
	growth_factor = scaler._growth_factor
	backoff_factor = scaler._backoff_factor
	growth_interval = scaler._growth_interval
	
	print(f"{prefix} –®–∞–≥ {step}:")
	print(f"   ‚Ä¢ Scale: {scale:.4e}")
	print(f"   ‚Ä¢ Growth factor: {growth_factor}")
	print(f"   ‚Ä¢ Backoff factor: {backoff_factor}")
	print(f"   ‚Ä¢ Growth interval: {growth_interval}")
	
	# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
	if scale > 1e6 or scale < 1e-6:
		print(f"   ‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π scale!")
		return False
	
	return True

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ:
if global_step % 50 == 0 and USE_AMP:
	monitor_scaler_state(global_step, GRAD_SCALER, "   üéõÔ∏è ")

def handle_nan_loss(loss_value, step_info):
	"""
	–û–±—Ä–∞–±–æ—Ç–∫–∞ NaN loss —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º scaler
	
	Args:
		loss_value: –∑–Ω–∞—á–µ–Ω–∏–µ loss (–º–æ–∂–µ—Ç –±—ã—Ç—å NaN)
		step_info: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ (global_step, accumulation_count –∏ —Ç.–¥.)
	"""
	# –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ NaN
	step_info['nan_count'] += 1
	
	if step_info['nan_count'] >= MAX_NAN_TOLERANCE:
		# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ NaN - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
		reload_checkpoint()
		
		# –ü–æ–ª–Ω—ã–π —Å–±—Ä–æ—Å scaler
		if USE_AMP:
			step_info['scaler'] = torch.cuda.amp.GradScaler(enabled=True)
		
		step_info['nan_count'] = 0
		step_info['accumulation'] = 0
		return 'reload'
	
	# –ü—Ä–æ–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º scaler
	step_info['optimizer'].zero_grad()
	step_info['accumulation'] = 0
	
	if USE_AMP and step_info['scaler']._scale is not None:
		# –í–∞–∂–Ω–æ: –æ–±–Ω–æ–≤–ª—è–µ–º scaler –¥–∞–∂–µ –ø—Ä–∏ –ø—Ä–æ–ø—É—Å–∫–µ —à–∞–≥–∞
		step_info['scaler'].update()
	
	return 'skip'

class TrainingState:
	"""–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å AMP"""
	
	def __init__(self, use_amp=False):
		self.use_amp = use_amp
		self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
		self.accumulation_count = 0
		self.nan_loss_count = 0
		self.max_nan_losses = 3
		
	def handle_nan(self, optimizer, model):
		"""–û–±—Ä–∞–±–æ—Ç–∫–∞ NaN —Å –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ scaler"""
		self.nan_loss_count += 1
		
		if self.nan_loss_count >= self.max_nan_losses:
			# –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
			self._reload_checkpoint(model, optimizer)
			return 'reload'
		
		# –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ –±–∞—Ç—á–∞
		optimizer.zero_grad()
		self.accumulation_count = 0
		
		# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ scaler –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
		if self.use_amp:
			try:
				self.scaler.update()
				print(f"   üîÑ Scaler –æ–±–Ω–æ–≤–ª—ë–Ω –ø–æ—Å–ª–µ NaN (—Å–æ—Å—Ç–æ—è–Ω–∏–µ: {self.scaler.get_scale():.4e})")
			except:
				# –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ scaler –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ
				self.scaler = torch.cuda.amp.GradScaler(enabled=True)
		
		return 'skip'
	
	def _reload_checkpoint(self, model, optimizer):
		"""–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Å –ø–æ–ª–Ω—ã–º —Å–±—Ä–æ—Å–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
		# –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
		# ...
		
		# –ü–æ–ª–Ω—ã–π —Å–±—Ä–æ—Å scaler
		if self.use_amp:
			self.scaler = torch.cuda.amp.GradScaler(enabled=True)
		
		self.nan_loss_count = 0
		self.accumulation_count = 0



# ================= –£–õ–£–ß–®–ï–ù–ù–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï =================

def save_checkpoint(model, tokenizer, optimizer, step, loss, epoch, checkpoint_dir, 
					is_best=False, scheduler=None, monitor=None):
	"""
	–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
	–ß–µ–∫–ø–æ–∏–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ float32 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —à–ª–æ –≤ mixed precision.
	"""
	try:
		checkpoint_dir = Path(checkpoint_dir)
		checkpoint_dir.mkdir(parents=True, exist_ok=True)
		
		print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ —à–∞–≥ {step}...")
		
		# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
		model_to_save.save_pretrained(str(checkpoint_dir))
		tokenizer.save_pretrained(str(checkpoint_dir))
		
		# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
		checkpoint_state = {
			'step': step,
			'epoch': epoch,
			'model_state_dict': model.state_dict(),
			'optimizer_state_dict': optimizer.state_dict(),
			'loss': float(loss),
			'precision_mode': 'amp_16bit' if USE_AMP else 'full_32bit',
			'batch_size': BATCH_SIZE,
			'learning_rate': LEARNING_RATE,
			'timestamp': datetime.now().isoformat(),
		}
		
		if scheduler:
			checkpoint_state['scheduler_step'] = scheduler.current_step
		
		if monitor and monitor.quality_scores:
			checkpoint_state['last_quality'] = monitor.quality_scores[-1] if monitor.quality_scores else None
		
		torch.save(checkpoint_state, checkpoint_dir / "checkpoint.pt")
		
		# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
		info_file = checkpoint_dir / "checkpoint_info.txt"
		with open(info_file, 'w', encoding='utf-8') as f:
			f.write(f"–ß–ï–ö–ü–û–ò–ù–¢ {step}\n")
			f.write(f"–≠–ø–æ—Ö–∞: {epoch}\n")
			f.write(f"Loss: {loss:.6f}\n")
			f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
			if is_best:
				f.write("\n üèÜ –°–¢–ê–¢–£–°: –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨\n")
		
		print("    ‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω")
		return True
		
	except Exception as e:
		print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
		return False

def load_last_checkpoint(checkpoint_dir, model, optimizer=None):
	"""–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
	try:
		checkpoint_dir = Path(checkpoint_dir)
		checkpoints = sorted(checkpoint_dir.glob("step_*"), 
						   key=lambda x: int(x.name.split('_')[1]) if x.name.split('_')[1].isdigit() else 0,
						   reverse=True)
		
		if checkpoints:
			last_checkpoint = checkpoints[0]
			checkpoint = torch.load(last_checkpoint / "checkpoint.pt", map_location='cpu')
			
			model.load_state_dict(checkpoint['model_state_dict'])
			if optimizer and 'optimizer_state_dict' in checkpoint:
				optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
			
			print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {last_checkpoint.name}")
			return checkpoint['step'], checkpoint['loss'], checkpoint['epoch']
	
	except Exception as e:
		print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
	
	return 0, float('inf'), 0

# ================= –ü–£–¢–ò =================
try:
	with open('paths.json', 'r') as pa:
		base_paths = json.load(pa)
except Exception as e:
	print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ paths.json: {e}")
	sys.exit(1)

BASE_DIR = Path(base_paths.get('base_dir'))
CHECKPOINTS_DIR = base_paths.get('checks_dir')
FINAL_MODEL_DIR = base_paths.get('final_model_dir')
LOGS_DIR = base_paths.get('logs_dir')
DATA_DIR = base_paths.get('data_dir')

CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ä–µ–∂–∏–º–æ–º
monitor = AdvancedTrainingMonitor(LOGS_DIR, tokenizer, debug_mode=DEBUG_MODE)

# –¢–æ–ª—å–∫–æ –≤ –æ—Ç–ª–∞–¥–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
if DEBUG_MODE >= 2:
	print("\n üîç –ü–†–û–í–ï–†–ö–ê –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê:")
	print(f"   ‚Ä¢ log_dir: {monitor.log_dir}")
	
	# –¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–ø–∏—Å—å
	monitor.save_to_csv(0, 1.0, 1e-4, 5.0, "TEST", 10.0, 0.5)

# –ü–†–û–í–ï–†–ö–ê –°–†–ê–ó–£ –ü–û–°–õ–ï –°–û–ó–î–ê–ù–ò–Ø
print("\n üîç –ü–†–û–í–ï–†–ö–ê –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê:")
print(f"   ‚Ä¢ log_dir: {monitor.log_dir}")
print(f"   ‚Ä¢ –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {monitor.log_dir.exists()}")

# –¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–ø–∏—Å—å —á–µ—Ä–µ–∑ monitor
monitor.save_to_csv(0, 1.0, 1e-4, 5.0, "TEST", 10.0, 0.5)

# –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–∞–π–ª
csv_file = monitor.log_dir / "training_log.csv"
print(f"   ‚Ä¢ CSV —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {csv_file.exists()}")
if csv_file.exists():
	print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {csv_file.stat().st_size} –±–∞–π—Ç")
	with open(csv_file, 'r') as f:
		print(f"   ‚Ä¢ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{f.read()}")

# ================= –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• =================
print("\n üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤...")

data_file = base_paths.get('dataset_file')
if data_file.exists():
	with open(data_file, 'r', encoding='utf-8') as f:
		dialogues = json.load(f)
	
	print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogues)} –¥–∏–∞–ª–æ–≥–æ–≤")
	
	texts = [dialogue['text'] for dialogue in dialogues]
	
else:
	print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_file}")
	sys.exit(1)

# ================= –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø =================
print("\n üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")

tokenizer = GPT2Tokenizer.from_pretrained(base_paths.get('source_model_dir'))
if not Path(base_paths.get('source_model_dir')).exists():
	print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_paths.get('source_model_dir')}")
	sys.exit(1)
tokenizer.pad_token = tokenizer.eos_token

all_tokens = []
for text in texts:
	tokens = tokenizer.encode(
		text,
		max_length=MAX_LENGTH,
		truncation=True,
		padding='max_length',
		return_tensors='pt'
	)
	all_tokens.append(tokens)

all_tokens = torch.cat(all_tokens, dim=0)

# –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π Dataset
class TensorDataset(torch.utils.data.Dataset):
	def __init__(self, tensors):
		self.tensors = tensors
	def __len__(self):
		return len(self.tensors)
	def __getitem__(self, idx):
		return self.tensors[idx]

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
split_idx = int(0.85 * len(all_tokens))
train_dataset = TensorDataset(all_tokens[:split_idx])
val_dataset = TensorDataset(all_tokens[split_idx:])

# DataLoader —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º shuffle
train_loader = torch.utils.data.DataLoader(
	train_dataset, 
	batch_size=BATCH_SIZE,
	shuffle=True,  # ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ
	num_workers=0   # –î–ª—è –Ω–∞—á–∞–ª–∞ 0, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –µ—Å–ª–∏ –º–Ω–æ–≥–æ —è–¥–µ—Ä
)

print(f"   Train: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
print(f"   Validation: {len(val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

# ================= –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò =================
print("\n üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")

# –ë–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
model = GPTNeoForCausalLM.from_pretrained(
	base_paths.get('source_model_dir'),
	device_map="auto",
	torch_dtype=torch.float16 if USE_AMP else torch.float32,  # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ —Å—Ä–∞–∑—É –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
)

print(" ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# ================= –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† =================
print("\n ‚ö° –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞...")

optimizer = bnb.optim.AdamW8bit(
	model.parameters(),
	lr=LEARNING_RATE,
	betas=(0.9, 0.95),
	weight_decay=0.01,
)

# ================= –†–ê–°–ß–ï–¢ –®–ê–ì–û–í –ò –®–ï–î–£–õ–ï–† =================

# –ù–ê–ô–î–ò–¢–ï –≠–¢–£ –°–¢–†–û–ö–£ (~740) –ò –ò–°–ü–†–ê–í–¨–¢–ï:
total_batches = len(train_data) // BATCH_SIZE
# total_steps = (total_batches // GRADIENT_ACCUMULATION) * EPOCHS  # ‚ùå –°–¢–ê–†–û–ï

# ‚¨áÔ∏è –ù–û–í–û–ï:
if GRADIENT_ACCUMULATION > 0:
	total_steps = max(1, math.ceil((total_batches + GRADIENT_ACCUMULATION - 1) // GRADIENT_ACCUMULATION * EPOCHS))
else:
	total_steps = max(1, total_batches * EPOCHS)

print("\n üìà –ü–õ–ê–ù –û–ë–£–ß–ï–ù–ò–Ø:")
print(f"   ‚Ä¢ –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")

scheduler = OptimalScheduler(optimizer, total_steps, LEARNING_RATE, WARMUP_RATIO)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–Ω–µ–≥–æ —Å—Ç–æ–ø–ø–∏–Ω–≥–∞
checkpoint_steps = [25, 50, 100, 200, 400, 600, 800]
best_loss = float('inf')
best_model_step = 0
patience = 3
patience_counter = 0
previous_val_loss = float('inf')
min_delta = 0.001  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–∏–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ

# –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
nan_loss_count = 0
max_nan_losses = 3

# ================= –û–ë–£–ß–ï–ù–ò–ï –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò =================
print("\n üéØ –ù–ê–ß–ò–ù–ê–Æ –û–ë–£–ß–ï–ù–ò–ï –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò...")

with TrainingMode(model):  # ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç use_cache –∏ gradient_checkpointing
	print("    ‚Ä¢ –†–µ–∂–∏–º: –û–ë–£–ß–ï–ù–ò–ï")
	print(f"   ‚Ä¢ use_cache: {model.config.use_cache}")
	print(f"   ‚Ä¢ gradient_checkpointing: {model.is_gradient_checkpointing}")

global_step = 0
start_time = datetime.now()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
initial_checkpoint_dir = CHECKPOINTS_DIR / "initial_model"
save_checkpoint(model, tokenizer, optimizer, 0, float('inf'), 0, initial_checkpoint_dir)

for epoch in range(EPOCHS):
	print(f"\n{'='*60}")
	print(f"üìö –≠–ü–û–•–ê {epoch+1}/{EPOCHS}")
	print(f"{'='*60}")
	
	if USE_AMP:
		try:
	# –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ scaler –≤ –≤–∞–ª–∏–¥–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
			current_scale = GRAD_SCALER.get_scale()
			if current_scale <= 0 or math.isnan(current_scale) or math.isinf(current_scale):
				print(f"   ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π scale ({current_scale:.2e}), –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º scaler")
				GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=True)
				print(f"   ‚úÖ –ù–æ–≤—ã–π scale: {GRAD_SCALER.get_scale():.2e}")
		except Exception as e:
			print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ scaler: {e}, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º")
			GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=True)
	# ================= –ö–û–ù–ï–¶ –ü–†–û–í–ï–†–ö–ò =================

	epoch_loss = 0.0
	batch_count = 0
	accumulation_count = 0
	epoch_start_time = time.time()
	last_print_time = time.time()
	
	# –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
	train_indices = torch.randperm(len(train_data))
	train_data_shuffled = train_data[train_indices]
	
	check_scaler_health()

	with TrainingMode(model):
		if HAS_TQDM and DEBUG_MODE <= 2:
    		pbar = tqdm(total=total_batches, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}", unit="–±–∞—Ç—á")
		for batch_idx in range(0, len(train_data_shuffled), BATCH_SIZE):
			total_batches = len(train_data_shuffled) // BATCH_SIZE

			# üõ°Ô∏è –ó–ê–©–ò–¢–ê SCALER –ü–ï–†–ï–î –ù–ê–ß–ê–õ–û–ú –≠–ü–û–•–ò
			if USE_AMP and epoch > 0:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ –≤—Ç–æ—Ä–æ–π —ç–ø–æ—Ö–∏
				scaler_ok = check_scaler_health(GRAD_SCALER, f"–≠–ø–æ—Ö–∞ {epoch+1}")
				if not scaler_ok:
					GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=True)
					print(f"   ‚úÖ Scaler –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω –ø–µ—Ä–µ–¥ —ç–ø–æ—Ö–æ–π {epoch+1}")

			step_start = time.time()

			if batch_idx + BATCH_SIZE > len(train_data_shuffled):
				continue
				
			
			batch = train_data_shuffled[batch_idx:batch_idx+BATCH_SIZE].cuda()
			if time.time() - step_start > MAX_STEP_TIME:
				print(f"‚ö†Ô∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ –∑–∞–Ω—è–ª–∞ {MAX_STEP_TIME} —Å–µ–∫—É–Ω–¥")
			batch_start_time = time.time()
			try:
				optimizer.zero_grad()

				with torch.cuda.amp.autocast(enabled=USE_AMP):
					outputs = model(batch, labels=batch)
					loss = outputs.loss
					loss_value = loss.item()

				# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
				if math.isnan(loss_value):
					nan_loss_count += 1
					print(f"   ‚ö†Ô∏è  NaN loss detected ({nan_loss_count}/{max_nan_losses})")
					
					if nan_loss_count >= max_nan_losses:
						print("    üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
						global_step, _, _ = load_last_checkpoint(CHECKPOINTS_DIR, model, optimizer)
						# –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ scaler –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
						if USE_AMP:
							GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=True)
						nan_loss_count = 0
						optimizer.zero_grad()
						accumulation_count = 0  # ‚¨ÖÔ∏è –°–ë–†–ê–°–´–í–ê–ï–ú accumulation_count
						continue
					
					# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –±–∞—Ç—á
					optimizer.zero_grad()
					accumulation_count = 0  # ‚¨ÖÔ∏è –°–ë–†–ê–°–´–í–ê–ï–ú accumulation_count
					# –ú—ã –Ω–µ –¥–µ–ª–∞–µ–º —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, –Ω–æ –¥–æ–ª–∂–Ω—ã –æ–±–Ω–æ–≤–∏—Ç—å scaler
					# –í–ê–ñ–ù–û: –ü—Ä–∏ –ø—Ä–æ–ø—É—Å–∫–µ —à–∞–≥–∞ –∏–∑-–∑–∞ NaN –æ–±–Ω–æ–≤–ª—è–µ–º scaler
					if USE_AMP and accumulation_count > 0:
						# –ú—ã –Ω–µ –¥–µ–ª–∞–µ–º —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, –Ω–æ –¥–æ–ª–∂–Ω—ã –æ–±–Ω–æ–≤–∏—Ç—å scaler
						# –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
						try:
							# –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥, –Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º scaler
							GRAD_SCALER.update()  # ‚¨ÖÔ∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
							print(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω scaler –ø–æ—Å–ª–µ –ø—Ä–æ–ø—É—Å–∫–∞ NaN –±–∞—Ç—á–∞")
						except Exception as e:
							print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ scaler: {e}")
							# –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º scaler –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
							GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=True)
					continue
				
				 # –û–ë–†–ê–¢–ù–´–ô –ü–†–û–•–û–î —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AMP –∏ —Å–∫–µ–π–ª–µ—Ä–æ–º
				epoch_loss += loss_value
				batch_count += 1
				
				# Gradient accumulation (–¥–µ–ª–∏–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π loss)
				accumulated_loss = loss / GRADIENT_ACCUMULATION
   
				if USE_AMP:
					GRAD_SCALER.scale(accumulated_loss).backward()
				else:
					accumulated_loss.backward()

				accumulation_count += 1
	
				if accumulation_count % GRADIENT_ACCUMULATION == 0:
					if USE_AMP:
						# –ü—Ä–∏–º–µ–Ω—è–µ–º gradient clipping –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º
						GRAD_SCALER.unscale_(optimizer)
						grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
		
						# –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —á–µ—Ä–µ–∑ —Å–∫–µ–π–ª–µ—Ä
						GRAD_SCALER.step(optimizer)
						try:
						# –ü—Ä–æ–≤–µ—Ä–∫–∞ scale –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
						current_scale = GRAD_SCALER.get_scale()
						if current_scale > 1e6 or current_scale < 1e-6:
							print(f"   ‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π scale: {current_scale:.2e}, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º")
							GRAD_SCALER = torch.cuda.amp.GradScaler(enabled=True)
					except:
						pass
						GRAD_SCALER.update()  # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞—Å—à—Ç–∞–± –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
					else:
						grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
						optimizer.step()

					current_lr, phase = scheduler.step()
					optimizer.zero_grad()
		
					global_step += 1

					if HAS_TQDM and DEBUG_MODE <= 2:
        				pbar.update(1)
				# ================= –í–´–í–û–î –ü–†–û–ì–†–ï–°–°–ê =================
				current_lr = LEARNING_RATE  # –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

				current_time = time.time()
				if current_time - last_print_time > 10:  # –ö–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
					progress = (batch_idx / len(train_data_shuffled)) * 100
					avg_loss_so_far = epoch_loss / (batch_count + 1e-8)
	
					# –†–ê–°–ß–ï–¢ –°–ö–û–†–û–°–¢–ò
					elapsed_since_last_print = current_time - last_print_time
					batches_since_last_print = (batch_idx // BATCH_SIZE) - last_batch_count if 'last_batch_count' in locals() else 1
					last_batch_count = batch_idx // BATCH_SIZE
	
					dialogs_per_second = batches_since_last_print * BATCH_SIZE / elapsed_since_last_print if elapsed_since_last_print > 0 else 0
					tokens_per_second = dialogs_per_second * MAX_LENGTH  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –≤ —Ç–æ–∫–µ–Ω–∞—Ö
	
					if DEBUG_MODE == 1:
					# –¶–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥ (–µ—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
						try:
						# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
							if dialogs_per_second > 0.5:
								speed_color = "\033[92m"  # –∑–µ–ª–µ–Ω—ã–π
								speed_icon = "üöÄ"
							elif dialogs_per_second > 0.2:
								speed_color = "\033[93m"  # –∂–µ–ª—Ç—ã–π
								speed_icon = "‚ö°"
							else:
								speed_color = "\033[91m"  # –∫—Ä–∞—Å–Ω—ã–π
								speed_icon = "üêå"
			
							reset_color = "\033[0m"
			
							print(f"\r   üîÑ {progress:5.1f}% | üìâ {loss_value:7.4f} | üéõÔ∏è {current_lr:.1e} | üß∫ {batch_idx//BATCH_SIZE:4d} | {speed_icon} {speed_color}{dialogs_per_second:5.2f} –¥–∏–∞–ª/—Å{reset_color}", end='', flush=True)
						except:
							# –ë–µ–∑ —Ü–≤–µ—Ç–æ–≤ –µ—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
							print(f"\r   üîÑ {progress:5.1f}% | Loss: {loss_value:7.4f} | LR: {current_lr:.2e} | –ë–∞—Ç—á: {batch_idx//BATCH_SIZE:4d} | üöÄ {dialogs_per_second:5.2f} –¥/—Å", end='', flush=True)
	
					elif DEBUG_MODE == 2:
						# –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
						print(f"\n   ‚è∞ {datetime.now().strftime('%H:%M:%S')}")
						print(f"   üìç –ë–∞—Ç—á {batch_idx//BATCH_SIZE} ({progress:.1f}%)")
						print(f"   üìâ Loss: {loss_value:.4f} (—Å—Ä–µ–¥–Ω: {avg_loss_so_far:.4f})")
						print(f"   üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {dialogs_per_second:.2f} –¥/—Å (~{tokens_per_second/1000:.1f}K —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)")
						print(f"   üíæ GPU –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated()/1024**3:.1f} GB")
						print(f"   ‚ö° GPU –º–æ—â–Ω–æ—Å—Ç—å: {get_gpu_power()}W")  # –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ—â–Ω–æ—Å—Ç–∏
	
					last_print_time = current_time

				elif DEBUG_MODE == 1:
					# –ë—ã—Å—Ç—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–±–µ–∑ —Ä–∞—Å—á–µ—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏)
					progress = (batch_idx / len(train_data_shuffled)) * 100
					print(f"\r   üîÑ {progress:5.1f}% | Loss: {loss_value:7.4f} | LR: {current_lr:.2e} | –ë–∞—Ç—á: {batch_idx//BATCH_SIZE:4d} | ‚è≥...", end='', flush=True)
				# ================= –ö–û–ù–ï–¶ –í–´–í–û–î–ê –ü–†–û–ì–†–ï–°–°–ê =================
				
				# Gradient accumulation
				loss = loss / GRADIENT_ACCUMULATION
				loss.backward()
				
				accumulation_count += 1
				
				# Step —Å gradient accumulation
				if accumulation_count % GRADIENT_ACCUMULATION == 0:
					# Gradient clipping
					grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
					
					# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
					optimizer.step()
					current_lr, phase = scheduler.step()  # ‚¨ÖÔ∏è –¢–ï–ü–ï–†–¨ current_lr –æ–±–Ω–æ–≤–ª–µ–Ω
					optimizer.zero_grad()
					
					global_step += 1
					step_time = time.time() - batch_start_time
					
					# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
					memory_gb = torch.cuda.memory_allocated() / 1024**3
					monitor.log_batch(global_step, loss_value, current_lr, grad_norm, memory_gb, step_time, phase)
					
					# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
					if global_step % 10 == 0:
						avg_loss = epoch_loss / batch_count
						elapsed = (datetime.now() - start_time).seconds / 60
						
						print(f"\n   –®–∞–≥ {global_step} [{phase}]:")
						print(f"   ‚Ä¢ Loss: {loss_value:.4f} | Avg: {avg_loss:.4f}")
						print(f"   ‚Ä¢ LR: {current_lr:.2e}")
						print(f"   ‚Ä¢ –í—Ä–µ–º—è: {elapsed:.1f} –º–∏–Ω")
					
					# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
					if global_step % 50 == 0:
						quality_score, empathy_score, current_temp = monitor.advanced_quality_check(
							model, tokenizer, global_step, adaptive_temp=True
						)
						print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.2f} | –≠–º–ø–∞—Ç–∏—è: {empathy_score:.2f} | Temp: {current_temp:.3f}")
					
					# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
					if global_step in checkpoint_steps:
						checkpoint_dir = CHECKPOINTS_DIR / f"step_{global_step}_epoch_{epoch+1}"
						save_checkpoint(model, tokenizer, optimizer, global_step, 
									  epoch_loss/batch_count, epoch+1, checkpoint_dir, 
									  scheduler=scheduler, monitor=monitor)
				
			except torch.cuda.OutOfMemoryError:  # ‚Üê –î–û–ë–ê–í–¨–¢–ï –≠–¢–£ –°–¢–†–û–ö–£
				print("‚ö†Ô∏è  –ù–µ —Ö–≤–∞—Ç–∏–ª–æ –ø–∞–º—è—Ç–∏ GPU, –æ—á–∏—â–∞—é...")
				torch.cuda.empty_cache()
				continue

			# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—Å–µ–≥–æ —à–∞–≥–∞
			if time.time() - step_start > MAX_STEP_TIME:
				print(f"‚ö†Ô∏è  –í–µ—Å—å —à–∞–≥ –∑–∞–Ω—è–ª {MAX_STEP_TIME} —Å–µ–∫—É–Ω–¥")
	
	# ================= –ö–û–ù–ï–¶ –≠–ü–û–•–ò =================
	if DEBUG_MODE == 1:
		print()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å—Ç—Ä–æ–∫—É –ø–æ—Å–ª–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
	
	# –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏
	avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else float('inf')
	print(f"\n‚úÖ –≠–ü–û–•–ê {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
	print(f"   ‚Ä¢ Train Loss: {avg_epoch_loss:.4f}")
	print(f"   ‚Ä¢ –®–∞–≥–æ–≤: {global_step}")
	
	# –†–∞—Å—á–µ—Ç perplexity –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
	perplexity = monitor.calculate_perplexity(model, val_data, BATCH_SIZE, epoch=epoch+1)
	print(f"   ‚Ä¢ Perplexity: {perplexity:.2f}")
	
	# –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞–Ω–Ω–∏–π —Å—Ç–æ–ø–ø–∏–Ω–≥
	if previous_val_loss != float('inf'):
		improvement = previous_val_loss - perplexity
		
		if improvement < min_delta:
			patience_counter += 1
			print(f"   ‚ö†Ô∏è  –ú–∞–ª–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ perplexity ({improvement:.4f} < {min_delta}). Patience: {patience_counter}/{patience}")
		else:
			patience_counter = 0
			print(f"   ‚úÖ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ perplexity: {improvement:.4f}")
		
		if patience_counter >= patience:
			print(f"\nüö´ –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê: –Ω–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π {patience} —ç–ø–æ—Ö–∏ –ø–æ–¥—Ä—è–¥")
			break
	
	# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
	if perplexity < best_loss:
		best_loss = perplexity
		best_model_step = global_step
		
		best_dir = CHECKPOINTS_DIR / f"BEST_epoch_{epoch+1}_perplexity_{best_loss:.2f}"
		save_checkpoint(model, tokenizer, optimizer, global_step, 
					  best_loss, epoch+1, best_dir, is_best=True, 
					  scheduler=scheduler, monitor=monitor)
		print(f"   üèÜ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: perplexity={best_loss:.2f}")
	
	previous_val_loss = perplexity
	
	# –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç —ç–ø–æ—Ö–∏
	epoch_checkpoint_dir = CHECKPOINTS_DIR / f"epoch_{epoch+1}_final"
	save_checkpoint(model, tokenizer, optimizer, global_step, avg_epoch_loss, 
				   epoch+1, epoch_checkpoint_dir, scheduler=scheduler, monitor=monitor)
	monitor.flush()

# ================= –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò =================
print("\n üíæ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ 32-–±–∏—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ...")

try:
	# –ü—Ä–∏–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –∫ float32, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è
	model = model.float()  # –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç 32-–±–∏—Ç–Ω—ã–µ –≤–µ—Å–∞
	
	model.save_pretrained(str(FINAL_MODEL_DIR))
	tokenizer.save_pretrained(str(FINAL_MODEL_DIR))
	
	training_info = {
		'total_steps': global_step,
		'final_train_loss': avg_epoch_loss,
		'best_perplexity': best_loss,
		'best_step': best_model_step,
		'epochs_completed': epoch + 1,
		'early_stopped': patience_counter >= patience,
		'training_precision': 'float16 (AMP)' if USE_AMP else 'float32',
		'saved_in_precision': 'float32',  # –í—Å–µ–≥–¥–∞ 32 –±–∏—Ç–∞
		'final_perplexity': perplexity,
		'batch_size': BATCH_SIZE,
		'learning_rate': LEARNING_RATE,
		'training_time_minutes': (datetime.now() - start_time).seconds / 60,
		'completion_time': datetime.now().isoformat(),
		'adaptive_training': True,
		'advanced_metrics': True,
		'gradient_checkpointing': True,
		'use_cache_strategy': 'adaptive'
	}
	
	with open(FINAL_MODEL_DIR / "training_info.json", 'w', encoding='utf-8') as f:
		json.dump(training_info, f, ensure_ascii=False, indent=2)
	
	print(" ‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
	
except Exception as e:
	print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

# ================= –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ =================
print("\n üß™ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –° –ê–î–ê–ü–¢–ò–í–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò...")
with GenerationMode(model):  # ‚úÖ –ì–ï–ù–ï–†–ê–¶–ò–Ø: cache=ON, gc=OFF
	print("    ‚Ä¢ –†–µ–∂–∏–º: –ì–ï–ù–ï–†–ê–¶–ò–Ø")
	print(f"   ‚Ä¢ use_cache: {model.config.use_cache}")
	print(f"   ‚Ä¢ gradient_checkpointing: {model.is_gradient_checkpointing}")

test_prompts = [
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ù–µ –º–æ–≥—É –ø–µ—Ä–µ—Å—Ç–∞—Ç—å –≤–æ–ª–Ω–æ–≤–∞—Ç—å—Å—è.",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ß—É–≤—Å—Ç–≤—É—é —Å–µ–±—è –æ—á–µ–Ω—å –æ–¥–∏–Ω–æ–∫–æ.",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ö–∞–∫ –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –≤ –∂–∏–∑–Ω–∏?",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –í—Å—ë –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ, –Ω–µ –≤–∏–∂—É –ø—Ä–∏—á–∏–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å.",
	"–ü–∞—Ü–∏–µ–Ω—Ç: –ë–æ—é—Å—å, —á—Ç–æ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∏–∑–º–µ–Ω—é—Å—å."
]

for i, prompt in enumerate(test_prompts):
	try:
		# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
		last_quality = monitor.quality_scores[-1][1] if monitor.quality_scores else 0.5
		adaptive_temp = max(0.6, 0.9 - (last_quality * 0.3))
		
		with GenerationMode(model):  # –ö–∞–∂–¥—ã–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ
			response = monitor.generate_adaptive_response(model, tokenizer, prompt, adaptive_temp)
			score = monitor.evaluate_response_comprehensive(prompt, response)
			empathy_score = monitor.calculate_empathy_score(response)
		
		print(f"\n{i+1}. üí≠ {prompt}")
		print(f"   üå°Ô∏è  Temp: {adaptive_temp:.3f}")
		print(f"   üí¨ {response[:120]}{'...' if len(response) > 120 else ''}")
		print(f"   üìä –û—Ü–µ–Ω–∫–∞: {score:.2f} | –≠–º–ø–∞—Ç–∏—è: {empathy_score:.2f}")
		
	except Exception as e:
		print(f"\n{i+1}. ‚ùå –û—à–∏–±–∫–∞: {e}")

print(f"\n{'='*80}")
print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
print(f"{'='*80}")
print(" üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
print(f"   ‚Ä¢ –®–∞–≥–æ–≤: {global_step}")
print(f"   ‚Ä¢ –õ—É—á—à–∏–π perplexity: {best_loss:.2f}")
print(f"   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–π perplexity: {perplexity:.2f}")
print(f"   ‚Ä¢ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {'–î–∞' if patience_counter >= patience else '–ù–µ—Ç'}")
print(f"   ‚Ä¢ NaN –æ–±—Ä–∞–±–æ—Ç–æ–∫: {nan_loss_count}")
print(f"   ‚Ä¢ –í—Ä–µ–º—è: {(datetime.now() - start_time).seconds/60:.1f} –º–∏–Ω")
print("    ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∂–∏–º—ã:")
print(f"      - –û–±—É—á–µ–Ω–∏–µ: cache=OFF, gradient_checkpointing={model.is_gradient_checkpointing}")
print("       - –í–∞–ª–∏–¥–∞—Ü–∏—è: cache=OFF, gradient_checkpointing=OFF")
print("       - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: cache=ON, gradient_checkpointing=OFF")
print(f"{'='*80}")
